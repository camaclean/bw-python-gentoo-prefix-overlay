diff --git a/setup.py b/setup.py
index 0de3ed7..f75d719 100644
--- a/setup.py
+++ b/setup.py
@@ -89,17 +89,6 @@ dep_libs = [
 def build_libs(libs):
     for lib in libs:
         assert lib in dep_libs, 'invalid lib: {}'.format(lib)
-    build_libs_cmd = ['bash', 'torch/lib/build_libs.sh']
-    my_env = os.environ.copy()
-    my_env["PYTORCH_PYTHON"] = sys.executable
-    if WITH_SYSTEM_NCCL:
-        my_env["NCCL_ROOT_DIR"] = NCCL_ROOT_DIR
-    if WITH_CUDA:
-        my_env["CUDA_BIN_PATH"] = CUDA_HOME
-        build_libs_cmd += ['--with-cuda']
-
-    if subprocess.call(build_libs_cmd + libs, env=my_env) != 0:
-        sys.exit(1)
 
     if 'THNN' in libs or 'THCUNN' in libs:
         from tools.nnwrap import generate_wrappers as generate_nn_wrappers
@@ -123,8 +112,6 @@ class build_deps(Command):
             libs += ['nccl']
         libs += ['libshm', 'ATen', 'nanopb']
         if WITH_DISTRIBUTED:
-            if sys.platform.startswith('linux'):
-                libs += ['gloo']
             libs += ['THD']
         build_libs(libs)
 
@@ -188,16 +175,7 @@ def monkey_patch_THD_link_flags():
     THD's dynamic link deps are not determined until after build_deps is run
     So, we need to monkey-patch them in later
     '''
-    # read tmp_install_path/THD_deps.txt for THD's dynamic linkage deps
-    with open(tmp_install_path + '/THD_deps.txt', 'r') as f:
-        thd_deps_ = f.read()
-    thd_deps = []
-    # remove empty lines
-    for l in thd_deps_.split(';'):
-        if l != '':
-            thd_deps.append(l)
-
-    C.extra_link_args += thd_deps
+    pass
 
 
 class build_ext(setuptools.command.build_ext.build_ext):
@@ -266,11 +244,11 @@ class build_ext(setuptools.command.build_ext.build_ext):
         for d in (autograd_gen_dir, jit_gen_dir):
             if not os.path.exists(d):
                 os.mkdir(d)
-        gen_variable_type(
-            'torch/lib/build/ATen/ATen/Declarations.yaml',
+        gen_variable_type(os.environ.get('EPREFIX','') +
+            '/usr/share/ATen/Declarations.yaml',
             autograd_gen_dir)
-        gen_jit_dispatch(
-            'torch/lib/build/ATen/ATen/Declarations.yaml',
+        gen_jit_dispatch(os.environ.get('EPREFIX','') +
+            '/usr/share/ATen/Declarations.yaml',
             jit_gen_dir)
 
         # It's an old-style class in Python 2.7...
@@ -325,28 +303,17 @@ extra_compile_args = ['-std=c++11', '-Wno-write-strings',
                       '-Wno-missing-braces']
 
 cwd = os.path.dirname(os.path.abspath(__file__))
-lib_path = os.path.join(cwd, "torch", "lib")
-
-
-# Check if you remembered to check out submodules
-def check_file(f):
-    if not os.path.exists(f):
-        print("Could not find {}".format(f))
-        print("Did you run 'git submodule update --init'?")
-        sys.exit(1)
-check_file(os.path.join(lib_path, "gloo", "CMakeLists.txt"))
-check_file(os.path.join(lib_path, "nanopb", "CMakeLists.txt"))
-check_file(os.path.join(lib_path, "pybind11", "CMakeLists.txt"))
+lib_path = os.path.join(os.environ.get('EPREFIX',''), "usr", "lib64")
 
 tmp_install_path = lib_path + "/tmp_install"
+system_include = os.path.join(os.environ.get('EPREFIX',''), "usr/include")
 include_dirs += [
     cwd,
     os.path.join(cwd, "torch", "csrc"),
-    lib_path + "/pybind11/include",
-    tmp_install_path + "/include",
-    tmp_install_path + "/include/TH",
-    tmp_install_path + "/include/THNN",
-    tmp_install_path + "/include/ATen",
+    system_include + "/pybind11",
+    system_include + "/TH",
+    system_include + "/THNN",
+    system_include + "/ATen",
 ]
 
 library_dirs.append(lib_path)
@@ -464,7 +431,7 @@ if WITH_DISTRIBUTED:
             "torch/csrc/distributed/Storage.cpp",
         ]
         extra_compile_args += ['-DWITH_DISTRIBUTED_MW']
-    include_dirs += [tmp_install_path + "/include/THD"]
+    include_dirs += [system_include + "/THD"]
     main_link_args += [THD_LIB]
 
 if WITH_CUDA:
@@ -475,7 +442,7 @@ if WITH_CUDA:
         if os.path.exists(cuda_lib_path):
             break
     include_dirs.append(cuda_include_path)
-    include_dirs.append(tmp_install_path + "/include/THCUNN")
+    include_dirs.append(system_include + "/THCUNN")
     library_dirs.append(cuda_lib_path)
     extra_link_args.append('-Wl,-rpath,' + cuda_lib_path)
     extra_compile_args += ['-DWITH_CUDA']
@@ -533,11 +500,11 @@ if DEBUG:
     extra_compile_args += ['-O0', '-g']
     extra_link_args += ['-O0', '-g']
 
+CXXNAME = os.getenv('CXX', 'g++')
 if os.getenv('PYTORCH_BINARY_BUILD') and platform.system() == 'Linux':
     print('PYTORCH_BINARY_BUILD found. Static linking libstdc++ on Linux')
     # get path of libstdc++ and link manually.
     # for reasons unknown, -static-libstdc++ doesn't fully link some symbols
-    CXXNAME = os.getenv('CXX', 'g++')
     STDCPP_LIB = subprocess.check_output([CXXNAME, '-print-file-name=libstdc++.a'])
     STDCPP_LIB = STDCPP_LIB[:-1]
     if type(STDCPP_LIB) != str:  # python 3
@@ -585,7 +552,7 @@ THNN = Extension("torch._thnn._THNN",
                  extra_link_args=extra_link_args + [
                      TH_LIB,
                      THNN_LIB,
-                     make_relative_rpath('../lib'),
+#                     make_relative_rpath('../lib'),
                  ]
                  )
 extensions.append(THNN)
diff --git a/torch/_thnn/utils.py b/torch/_thnn/utils.py
index 66d527a..57111f8 100644
--- a/torch/_thnn/utils.py
+++ b/torch/_thnn/utils.py
@@ -2,8 +2,8 @@ import os
 import itertools
 import importlib
 
-THNN_H_PATH = os.path.join(os.path.dirname(__file__), '..', 'lib', 'THNN.h')
-THCUNN_H_PATH = os.path.join(os.path.dirname(__file__), '..', 'lib', 'THCUNN.h')
+THNN_H_PATH = os.path.join('@GENTOO_EPREFIX@', 'usr/include/THNN/generic', 'THNN.h')
+THCUNN_H_PATH = os.path.join('@GENTOO_EPREFIX@', 'usr/include/THCUNN/generic', 'THCUNN.h')
 
 
 def _unpickle_backend(backend_name):
diff --git a/torch/__init__.py b/torch/__init__.py
index 8517497..572e30b 100644
--- a/torch/__init__.py
+++ b/torch/__init__.py
@@ -262,7 +262,7 @@ from .functional import *
 
 def manager_path():
     import os
-    path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'lib', 'torch_shm_manager')
+    path = os.path.join('@GENTOO_EPREFIX@', 'usr/libexec', 'torch_shm_manager')
     if not os.path.exists(path):
         raise RuntimeError("Unable to find torch_shm_manager at " + path)
     return path.encode('utf-8')
